{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Practice 6] Lidar Object Detection\n",
    "* 0. 실습 환경 설정\n",
    "* 1. 데이터셋 준비하기\n",
    "* 2. Training\n",
    "* 3. Testing\n",
    "* 4. Inference\n",
    "    * 4-1.Import modules / 하이퍼파라미터 설정\n",
    "    * 4-2. 모델 불러오기\n",
    "    * 4-3. 2D 및 BEV Detection결과 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 실습 환경 설정\n",
    "* 가상환경명 : lidar\n",
    "* Python 버전 : python 3.8\n",
    "* 설치 목록\n",
    "    * !pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "    * opencv_python==4.1.0.25\n",
    "    * matplotlib\n",
    "* Complex-YOLO Git 주소 : https://github.com/maudzung/Complex-YOLOv4-Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Complex-YOLOv4-Pytorch'...\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/maudzung/Complex-YOLOv4-Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터셋 준비하기\n",
    "* 이론 PPT에 나와있는 것 참고하여 다음과 같이 Folder structure 구성하기\n",
    "```\n",
    "${ROOT}\n",
    "└── checkpoints/    \n",
    "    ├── complex_yolov3/\n",
    "    └── complex_yolov4/\n",
    "└── dataset/    \n",
    "    └── kitti/\n",
    "        ├──ImageSets/\n",
    "        │   ├── train.txt\n",
    "        │   └── val.txt\n",
    "        ├── training/\n",
    "        │   ├── image_2/ <-- for visualization\n",
    "        │   ├── calib/\n",
    "        │   ├── label_2/\n",
    "        │   └── velodyne/\n",
    "        └── testing/  \n",
    "        │   ├── image_2/ <-- for visualization\n",
    "        │   ├── calib/\n",
    "        │   └── velodyne/ \n",
    "        └── classes_names.txt\n",
    "└── src/\n",
    "    ├── config/\n",
    "    ├── cfg/\n",
    "        │   ├── complex_yolov3.cfg\n",
    "        │   ├── complex_yolov3_tiny.cfg\n",
    "        │   ├── complex_yolov4.cfg\n",
    "        │   ├── complex_yolov4_tiny.cfg\n",
    "    │   ├── train_config.py\n",
    "    │   └── kitti_config.py\n",
    "    ├── data_process/\n",
    "    │   ├── kitti_bev_utils.py\n",
    "    │   ├── kitti_dataloader.py\n",
    "    │   ├── kitti_dataset.py\n",
    "    │   ├── kitti_data_utils.py\n",
    "    │   ├── train_val_split.py\n",
    "    │   └── transformation.py\n",
    "    ├── models/\n",
    "    │   ├── darknet2pytorch.py\n",
    "    │   ├── darknet_utils.py\n",
    "    │   ├── model_utils.py\n",
    "    │   ├── yolo_layer.py\n",
    "    └── utils/\n",
    "    │   ├── evaluation_utils.py\n",
    "    │   ├── iou_utils.py\n",
    "    │   ├── logger.py\n",
    "    │   ├── misc.py\n",
    "    │   ├── torch_utils.py\n",
    "    │   ├── train_utils.py\n",
    "    │   └── visualization_utils.py\n",
    "    ├── evaluate.py\n",
    "    ├── test.py\n",
    "    ├── test.sh\n",
    "    ├── train.py\n",
    "    └── train.sh\n",
    "├── README.md \n",
    "└── requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TODO : yolov4.weights 다운로드 [링크](https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights)\n",
    "* TODO : ${root}/checkpoints 에 yolov4.weights 넣어주기\n",
    "* TODO : 경로 변경 (${root}/src 로 변경)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --gpu_idx 0  --pretrained_path ../checkpoints/yolov4.weights --cfgfile ./config/cfg/complex_yolov4.cfg --save_path ../checkpoints/Complex_yolo_yolo_v4.pth --num_epochs 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TODO : 경로 변경 (${root}/src 로 변경)\n",
    "* TODO : pretrained_path에 해당 [링크](https://drive.google.com/drive/folders/1RHD9PBvk-9SjbKwoi_Q1kl9-UGFo2Pth)의 가중치나 학습한 가중치 경로 넣어주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python test.py --gpu_idx 0 --pretrained_path ../checkpoints/complex_yolov4_mse_loss.pth --cfgfile ./config/cfg/complex_yolov4.cfg --save_test_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inference\n",
    "### 3-1.Import modules / 하이퍼파라미터 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TODO : 경로 변경 (${root}/src 로 변경)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.darknet2pytorch import Darknet\n",
    "from data_process import kitti_data_utils,kitti_bev_utils\n",
    "from utils.evaluation_utils import post_processing, rescale_boxes, post_processing_v2\n",
    "from utils.visualization_utils import show_image_with_boxes, merge_rgb_to_bev, predictions_to_kitti_format\n",
    "import config.kitti_config as cnf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TODO : weight_path에 해당 [링크](https://drive.google.com/drive/folders/1RHD9PBvk-9SjbKwoi_Q1kl9-UGFo2Pth)의 가중치나 학습한 가중치 경로 넣어주기\n",
    "* TODO : kitti dataset에서 0번만 빼서 ${root}/sample에 넣어주기\n",
    "    * ${root}/dataset/kitti/training/image_2/000000.png\n",
    "    * ${root}/dataset/kitti/training/label_2/000000.txt\n",
    "    * ${root}/dataset/kitti/training/velodyne/000000.bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_path = '../sample/000000.png'\n",
    "lidar_path = '../sample/000000.bin'\n",
    "calib_path = '../sample/000000.txt'\n",
    "\n",
    "cfg_file = './config/cfg/complex_yolov4.cfg'\n",
    "weight_path = \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "nms_thresh =0.5\n",
    "conf_thresh = 0.5\n",
    "img_size = 608"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] No error, the convolution haven't activate linear\n",
      "[INFO] No error, the convolution haven't activate linear\n",
      "[INFO] No error, the convolution haven't activate linear\n",
      "model loaded!\n"
     ]
    }
   ],
   "source": [
    "model = Darknet(cfgfile=cfg_file, use_giou_loss=False)\n",
    "model.load_state_dict(torch.load(weight_path, map_location=device))\n",
    "model = model.to(device=device)\n",
    "model.eval()\n",
    "print(\"model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. 2D 및 BEV Image Detection 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_data = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n",
    "b = kitti_bev_utils.removePoints(lidar_data, cnf.boundary)\n",
    "rgb_map = kitti_bev_utils.makeBVFeature(b, cnf.DISCRETIZATION, cnf.boundary)\n",
    "rgb_map = torch.from_numpy(rgb_map).unsqueeze(0).float()\n",
    "input_imgs = rgb_map.to(device=device).float()\n",
    "outputs = model(input_imgs)\n",
    "detections = post_processing_v2(outputs, conf_thresh=conf_thresh, nms_thresh=nms_thresh)\n",
    "\n",
    "img_detections = []  # Stores detections for each image index\n",
    "img_detections.extend(detections)\n",
    "\n",
    "img_bev = rgb_map.squeeze() * 255\n",
    "img_bev = img_bev.permute(1, 2, 0).numpy().astype(np.uint8)\n",
    "img_bev = cv2.resize(img_bev, (img_size, img_size))\n",
    "img_bev_v = img_bev.copy()\n",
    "\n",
    "for detections in img_detections:\n",
    "    if detections is None:\n",
    "        continue\n",
    "    # Rescale boxes to original image\n",
    "    detections = rescale_boxes(detections, img_size, img_bev.shape[:2])\n",
    "    for x, y, w, l, im, re, *_, cls_pred in detections:\n",
    "        yaw = np.arctan2(im, re)\n",
    "        # Draw rotated box\n",
    "        kitti_bev_utils.drawRotatedBox(img_bev, x, y, w, l, yaw, cnf.colors[int(cls_pred)])\n",
    "\n",
    "img_rgb = cv2.imread(rgb_path)\n",
    "calib = kitti_data_utils.Calibration(calib_path)\n",
    "objects_pred = predictions_to_kitti_format(img_detections, calib, img_rgb.shape, img_size)\n",
    "img_rgb = show_image_with_boxes(img_rgb, objects_pred, calib, False)\n",
    "\n",
    "img_bev_flip = cv2.flip(cv2.flip(img_bev, 0), 1)\n",
    "out_img = merge_rgb_to_bev(img_rgb, img_bev_flip, output_width=608)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lidar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
